{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a88d820",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setting Jupyter Schorcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45161e6b-d326-471d-b105-0ac5e8263b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Jupyter Notebook\n"
     ]
    }
   ],
   "source": [
    "print (\"First Jupyter Notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d7003a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "{\n",
    "\"shortcuts\": [\n",
    "        {\n",
    "            \"command\": \"notebook:hide-cell-outputs\",\n",
    "            \"keys\": [\n",
    "                \"H\"\n",
    "            ],\n",
    "            \"selector\": \".jp-Notebook:focus\"\n",
    "        },\n",
    "    {\n",
    "            \"command\": \"notebook:show-cell-outputs\",\n",
    "            \"keys\": [\n",
    "                \"S\"\n",
    "            ],\n",
    "            \"selector\": \".jp-Notebook:focus\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea684f2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download Folder In Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0626eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "In conda python wrinte the command \"!tar chvfz notebook.tar.gz ../*\" or \"!tar chvfz notebook.tar.gz ../*\"\n",
    "Reference: https://stackoverflow.com/questions/43042793/download-all-files-in-a-path-on-jupyter-notebook-server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a450f2",
   "metadata": {},
   "source": [
    "## Install Pandas in Pyspark and Use It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bc27de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"pandas==0.25.1\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff95466",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469b3c6f",
   "metadata": {},
   "source": [
    "## List All Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac45f13d",
   "metadata": {},
   "source": [
    "## Ploting In Pyspark: Installation Did Not Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"Cython\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ea0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"pyspark_dist_explore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad97f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark_dist_explore import hist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "hist(ax, my_df.select('field_1'), bins = 20, color=['red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805126b",
   "metadata": {},
   "source": [
    "## Create an Empty Pandas DF and Convert to Spark DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78ede942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas\n",
    "dfp = pandas.DataFrame(rows_list, columns = ['Id1', 'Id2',\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "028aa2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfps = spark.createDataFrame(dfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530fdb3e",
   "metadata": {},
   "source": [
    "## Create an Empty Spark Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f038a5",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbfb2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_leaves = spark.createDataFrame([('a','a','b','a'), ('a','b','b','a')], ['keywords','Threshold_Browse_Index','Threshold_Browse_Score','Threshold_Browse_Level'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4febaf",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2812766",
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "schema = StructType([\n",
    "  StructField('keywords', StringType(), True),\n",
    "  StructField('Threshold_Browse_Index', StringType(), True),\n",
    "  StructField('Threshold_Browse_Score', StringType(), True),\n",
    "  StructField('Threshold_Browse_Level', StringType(), True)\n",
    "  ])\n",
    "\n",
    "df_concat_leaves = spark.createDataFrame(emptyRDD, schema)\n",
    "print(df_concat_leaves.show())\n",
    "print(df_concat_leaves.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147b461c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Starting Spark Application and Set Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eda6396",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "import olympia\n",
    "olympia.cluster.connect(name = \"dgoyl2\")\n",
    "\n",
    "\n",
    "# To able to read bad files\n",
    "config_dict = {\n",
    "\"spark.sql.hive.manageFilesourcePartitions\":\"false\"\n",
    "}\n",
    "\n",
    "# For Cannot broadcast the table that is larger than 8GB or broadcast timeout\n",
    "config_dict = {\n",
    "\"spark.sql.autoBroadcastJoinThreshold\":-1\n",
    "}\n",
    "\n",
    "\n",
    "# To Increase broadcast timeout\n",
    "config_dict = {\n",
    "\"spark.sql.broadcastTimeout\": 2000       # I think the default is 300\n",
    "}\n",
    "\n",
    "# Other Unknown Config\n",
    "config_dict = {\n",
    "\"spark.sql.files.ignoreCorruptFiles\":\"true\",\n",
    "\"spark.hadoop.fs.s3.cse.enabled\":\"true\",\n",
    "\"spark.hadoop.fs.s3e.kms.id\" : \"arn:aws:kms:us-east-1:799645556480:key/dcda7203-d69b-4014-a560-377f9bbd3b5f\"\n",
    "}\n",
    "\n",
    "# Updating The Config\n",
    "olympia.cluster.update_session_config(config_dict)\n",
    "\n",
    "\n",
    "%spark\n",
    "\n",
    "%info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5985b2f1",
   "metadata": {},
   "source": [
    "## Enable Timer in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb922dd",
   "metadata": {},
   "source": [
    "#### Instalation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e83754c2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pip install ipython-autotime #In local jupyter kernel\n",
    "sc.install_pypi_package(\"ipython-autotime\") #In pyspark kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b6b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d40fec83",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 252 µs (started: 2022-06-08 04:48:13 +00:00)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "395e85c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 63.9 ms (started: 2022-06-08 04:48:14 +00:00)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,1000000):\n",
    "    j = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef074901",
   "metadata": {},
   "source": [
    "## If Olympia Error Appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8841e4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Use The Following Command In Terminal\n",
    "'''\n",
    "find ./olympia/ -type f -name \"*.txt\" -exec sed -i 's/s3transfer<0.7.0,>=0.6.0/s3transfer<0.7.0,>=0.5.0/g' {} \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402ac25b",
   "metadata": {},
   "source": [
    "## Cluster Configration: Sweta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ca199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import time\n",
    "\n",
    "# import pyspark functions\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import HiveContext, SparkSession\n",
    "# from pyspark.sql.functions import *\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import regexp_replace,regexp_extract, col\n",
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lit\n",
    "#import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import time\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "conf=spark.sparkContext.getConf()\n",
    "conf.set('spark.python.worker.memory', '9192m')\n",
    "conf.set('spark.executor.memory', '64G')\n",
    "conf.set('spark.executor.cores', 8)\n",
    "conf.set('spark.driver.memory', '32G')\n",
    "conf.set('spark.driver.cores', 8)\n",
    "#conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "conf.set(\"spark.network.timeout\", \"180s\")\n",
    "conf.set(\"spark.rpc.message.maxSize\", \"1024\")\n",
    "conf.set(\"spark.rpc.retry.wait\", \"60s\")\n",
    "conf.set(\"spark.rdd.compress\", \"false\")\n",
    "# conf.set('spark.kryoserializer.buffer.max', '1024m')\n",
    "conf.set('sc.defaultParallelism', 10000)\n",
    "conf.set(\"spark.driver.maxResultSize\", \"0\")\n",
    "conf.set(\"spark.sql.shuffle.partitions\", 500)\n",
    "conf.set(\"spark.sql.hive.metastore.client.factory.class\",\"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\")\n",
    "# conf.set(\"spark.sql.hive.convertMetastoreParquet\", False)\n",
    "conf.set(\"spark.sql.hive.metastorePartitionPruning\", True)\n",
    "conf.set(\"fs.s3.cse.enabled\", False)\n",
    "\n",
    "conf.set( \"spark.pyspark.virtualenv.enabled\", \"true\")\n",
    "conf.set( \"spark.pyspark.virtualenv.type\",\"native\")\n",
    "conf.set( \"spark.pyspark.virtualenv.bin.path\",\"/usr/bin/virtualenv\")\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EIP_SG1\").config(conf=conf).enableHiveSupport().getOrCreate()\n",
    "\n",
    "try:\n",
    "    spark.sparkContext.install_pypi_package(\"matplotlib\")\n",
    "    spark.sparkContext.install_pypi_package(\"pandas\")\n",
    "    spark.sparkContext.install_pypi_package(\"numpy\")\n",
    "    spark.sparkContext.install_pypi_package(\"findspark\")\n",
    "    pass\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "# spark.sparkContext.install_pypi_package(\"pandas==0.25.1\")\n",
    "sqlContext = SQLContext(spark)\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe367c",
   "metadata": {},
   "source": [
    "## Rename S3 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "027a9b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket = \"span-dev-users\"\n",
    "old_file_key = \"dgoyl/Try_EMR/using_emb/conflated_output_17M_qba_hybrid_distribution_\"\\\n",
    "                            + t_string + \"/block_size_\" + str(max_block_size) + \"_IBT_Augment_sample\"\\\n",
    "+ \"/part-00000-d47425dc-6ce7-41ce-8054-cf23517aa32f-c000.tsv\"\n",
    "\n",
    "new_file_key = \"dgoyl/Try_EMR/using_emb/conflated_output_17M_qba_hybrid_distribution_\"\\\n",
    "                            + t_string + \"/block_size_\" + str(max_block_size) + \"_IBT_Augment_sample\"\\\n",
    "+ \"/part-00000-d47425dc-6ce7-41ce-8054-cf23517aa32f-c000.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7592effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "s3.Object(my_bucket,new_file_key).copy_from(CopySource = my_bucket  + \"/\" + old_file_key)\n",
    "s3.Object(my_bucket,old_file_key).delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
