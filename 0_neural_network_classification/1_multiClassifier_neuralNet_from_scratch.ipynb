{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63df294b-7dfd-4f68-9ea3-d8da848082d5",
   "metadata": {},
   "source": [
    "## ==========  MULTI CLASSIFICATION: CLEVR DATA SET  =============="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9555aa83-223b-43b9-8223-212a9d5b0bfa",
   "metadata": {},
   "source": [
    "### Reading Input Dataset: Image Pixels & Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06708265-b6bd-4f8d-b08e-386746376077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097128b3-2479-483f-89bb-975139f19f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data det:           [[ 0.21568627  0.21568627  0.21568627 ... -0.19215686 -0.15294118\n",
      "  -1.        ]\n",
      " [ 0.21568627  0.21568627  0.22352941 ... -0.19215686 -0.15294118\n",
      "  -1.        ]\n",
      " [ 0.21568627  0.21568627  0.22352941 ... -0.2        -0.15294118\n",
      "  -1.        ]\n",
      " ...\n",
      " [ 0.21568627  0.21568627  0.21568627 ... -0.19215686 -0.14509804\n",
      "  -1.        ]\n",
      " [ 0.21568627  0.21568627  0.21568627 ... -0.19215686 -0.14509804\n",
      "  -1.        ]\n",
      " [ 0.21568627  0.21568627  0.21568627 ... -0.19215686 -0.15294118\n",
      "  -1.        ]]\n",
      "\n",
      "training class labels:     [1 3 4 ... 4 3 2]\n",
      "\n",
      "testing data det:          [[ 0.20784314  0.21568627  0.21568627 ... -0.19215686 -0.15294118\n",
      "  -1.        ]\n",
      " [ 0.21568627  0.21568627  0.22352941 ... -0.19215686 -0.15294118\n",
      "  -1.        ]\n",
      " [ 0.20784314  0.21568627  0.21568627 ... -0.19215686 -0.15294118\n",
      "  -1.        ]\n",
      " ...\n",
      " [ 0.21568627  0.21568627  0.21568627 ... -0.19215686 -0.14509804\n",
      "  -1.        ]\n",
      " [ 0.21568627  0.21568627  0.21568627 ... -0.19215686 -0.14509804\n",
      "  -1.        ]\n",
      " [ 0.21568627  0.21568627  0.21568627 ... -0.19215686 -0.15294118\n",
      "  -1.        ]]\n",
      "\n",
      "testing class labels:      [1 4 2 2 2 1 1 2 1 5 3 5 3 1 2 1 1 1 2 4 4 3 1 3 4 5 2 3 1 1 5 1 5 4 2 1 2\n",
      " 5 1 1 3 2 2 3 5 4 3 4 4 1 5 3 2 2 3 4 2 2 5 3 1 1 5 2 4 4 3 3 1 3 5 2 1 3\n",
      " 3 1 4 2 4 5 3 4 1 3 1 2 5 4 3 4 5 5 4 5 4 1 3 3 1 3 2 1 1 5 3 5 1 4 2 4 5\n",
      " 1 4 1 5 2 3 5 2 5 2 5 3 2 3 3 3 3 5 3 1 2 4 3 2 4 5 4 3 5 4 4 3 4 4 2 2 1\n",
      " 1 1 4 1 1 5 4 1 2 4 3 4 1 3 4 1 3 5 1 2 1 3 1 1 3 4 2 2 5 5 2 1 4 3 5 5 4\n",
      " 3 2 5 5 3 1 2 4 1 3 4 3 2 4 1 4 4 5 1 2 3 3 2 5 4 4 3 3 4 3 5 5 1 3 3 4 5\n",
      " 3 3 1 5 4 1 3 2 5 2 3 5 2 4 2 3 3 3 3 5 5 5 5 5 1 2 5 2 5 1 1 4 5 1 1 3 3\n",
      " 4 4 5 1 2 3 1 5 1 1 2 5 5 2 3 4 4 3 1 4 3 4 1 2 3 1 3 3 3 4 5 2 1 1 3 3 5\n",
      " 1 3 2 3 1 5 3 5 1 5 1 3 2 5 4 2 4 1 2 3 5 4 2 4 2 4 3 2 3 5 3 5 2 4 5 2 1\n",
      " 3 5 2 4 5 3 3 3 5 2 2 3 4 1 5 5 2 4 1 3 5 2 3 4 1 3 3 1 1 1 2 4 2 4 2 3 5\n",
      " 5 3 1 5 5 1 3 4 1 4 1 1 4 4 1 2 5 3 4 3 1 3 2 3 2 3 2 4 4 3 2 1 2 4 5 4 2\n",
      " 3 3 1 5 5 4 4 3 1 2 1 4 1 2 5 2 4 1 1 4 3 5 2 4 5 1 2 3 3 5 4 1 4 3 1 5 5\n",
      " 4 5 5 4 5 4 2 1 2 5 2 4 3 1 2 1 3 2 2 4 4 2 1 3 5 3 2 5 2 3 1 5 5 3 2 5 3\n",
      " 3 5 5 3 3 4 2 5 2 2 5 1 1 3 1 2 1 4 5 2 5 3 4 1 3 5 4 1 4 3 1 2 4 1 3 1 1\n",
      " 4 1 2 2 1 2 4 3 5 1 1 5 1 4 1 5 5 5 3 5 1 4 2 1 5 1 2 4 5 1 2 3 5 4 5 3 4\n",
      " 5 1 5 1 4 3 1 1 2 3 4 3 4 5 4 3 3 5 2 3 1 5 4 2 2 3 3 2 3 2 1 5 2 3 1 3 5\n",
      " 3 1 2 4 5 5 2 2 1 1 4 3 5 5 5 4 1 1 3 3 5 1 4 2 3 5 1 2 3 1 3 1 3 5 4 4 4\n",
      " 1 1 2 5 2 3 3 4 5 4 4 3 5 1 3 3 2 2 5 3 2 4 2 1 4 3 3 1 1 1 3 2 3 4 2 1 2\n",
      " 1 4 2 4 4 2 5 2 3 5 1 5 4 1 5 5 1 3 5 4 3 5 3 2 5 4 4 2 3 4 4 5 1 3 3 5 2\n",
      " 3 4 5 4 1 2 5 2 5 2 3 3 4 4 1 4 4 5 2 5 5 2 1 1 1 5 2 4 3 3 3 1 2 2 1 4 3\n",
      " 2 4 2 2 5 1 5 1 2 2 5 4 1 3 5 4 2 4 4 2 1 4 2 4 1 1 2 3 2 3 2 4 2 2 1 5 2\n",
      " 3 2 3 3 1 4 1 3 2 4 1 5 3 2 3 4 5 2 5 3 3 5 5 4 1 5 2 4 2 4 1 1 5 4 3 5 1\n",
      " 4 2 1 1 4 2 3 3 5 5 4 5 4 3 5 5 1 4 3 5 4 5 5 1 4 4 4 1 2 1 1 2 1 5 4 4 5\n",
      " 4 3 1 3 5 2 1 4 4 2 5 4 1 1 1 3 3 5 1 4 3 1 5 1 2 1 5 1 1 5 4 5 4 4 4 2 1\n",
      " 5 3 2 1 5 1 3 2 1 2 5 5 4 3 5 1 3 4 5 4 2 2 3 5 3 3 1 2 4 4 4 4 4 2 1 4 1\n",
      " 4 1 5 1 1 3 3 3 2 2 2 1 1 3 4 1 3 5 4 5 1 2 2 3 4 1 3 4 3 1 2 2 1 4 5 3 1\n",
      " 1 3 5 1 3 2 1 1 2 3 2 1 3 3 3 3 3 5 3 4 4 3 1 2 3 2 3 3 5 4 3 3 3 5 5 5 1\n",
      " 3]\n",
      "\n",
      "size of single sample:       1024\n",
      "number of training samples:  10000\n",
      "number of testing  samples:  (10000, 1024)\n",
      "number of testing  samples:  (1000, 1024)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import copy\n",
    "import sys\n",
    "import pdb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def get_data(x_path, y_path, doNormalize):\n",
    "    ''' Returns: x: np array of [NUM_OF_SAMPLES x n]  y: np array of [NUM_OF_SAMPLES]\n",
    "    '''\n",
    "    x = np.load(x_path)\n",
    "    y = np.load(y_path)\n",
    "    \n",
    "\n",
    "    y = y.astype('int')\n",
    "    x = x.astype('float')\n",
    "\n",
    "    #normalize x:\n",
    "    if doNormalize:\n",
    "        x = 2*(0.5 - x/255)\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = get_data(\"/Users/dishantgoyal/Desktop/ml/0_neural_network_classification/dataset_multiClass_clevr/x_train.npy\", \"/Users/dishantgoyal/Desktop/ml/0_neural_network_classification/dataset_multiClass_clevr/y_train.npy\", 1)\n",
    "x_test,   y_test = get_data(\"/Users/dishantgoyal/Desktop/ml/0_neural_network_classification/dataset_multiClass_clevr/x_test.npy\", \"/Users/dishantgoyal/Desktop/ml/0_neural_network_classification/dataset_multiClass_clevr/y_test.txt\", 1)\n",
    "print (\"training data det:          \", x_train)\n",
    "print (\"\\ntraining class labels:    \", y_train)\n",
    "print (\"\\ntesting data det:         \", x_test)\n",
    "print (\"\\ntesting class labels:     \", y_test)\n",
    "print (\"\\nsize of single sample:      \", len(x_train[0]))\n",
    "print (\"number of training samples: \", len(x_train))\n",
    "print (\"number of testing  samples: \", x_test.shape)\n",
    "x_test = x_test[0:1000]\n",
    "print (\"number of testing  samples: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21e919b-f294-4873-8a45-ae46c248f438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9b6d286-6a63-41e7-bd4f-085d44b12ce3",
   "metadata": {},
   "source": [
    "### Example Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "810a40c2-c5cd-4686-b18f-e616bcac8759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of boxes in the image:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train_rgb, y_train = get_data(\"/Users/dishantgoyal/Desktop/ml/0_neural_network_classification/dataset_multiClass_clevr/x_train.npy\", \"/Users/dishantgoyal/Desktop/ml/0_neural_network_classification/dataset_multiClass_clevr/y_train.npy\", 0)\n",
    "x_test_rgb,  y_test = get_data(\"/Users/dishantgoyal/Desktop/ml/0_neural_network_classification/dataset_multiClass_clevr/x_test.npy\", \"/Users/dishantgoyal/Desktop/ml/0_neural_network_classification/dataset_multiClass_clevr/y_test.txt\", 0)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "index = 4\n",
    "img = np.reshape(np.uint64(x_train_rgb[index]), (16, 16, 4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Turn off the axis\n",
    "plt.show\n",
    "print (\"number of boxes in the image: \", y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71020057-2723-445e-845e-61cb18d7a0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16b15223-ad40-4702-becb-83761b04cf76",
   "metadata": {},
   "source": [
    "### Distribution of Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7959e81b-6b7a-43b6-a9c8-d99f9f9d5b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] unique labels:       [1 2 3 4 5]\n",
      "[train] count distribution:  [   0 1971 1978 1952 2008 2091]\n",
      "[test]  unique labels:       [1 2 3 4 5]\n",
      "[test]  count distribution:  [  0 214 183 219 191 193]\n"
     ]
    }
   ],
   "source": [
    "unique_labels  = np.unique(y_train)\n",
    "label_count    = np.bincount(np.int64(y_train))\n",
    "\n",
    "print (\"[train] unique labels:      \", unique_labels)\n",
    "print (\"[train] count distribution: \", label_count)\n",
    "\n",
    "unique_labels  = np.unique(y_test)\n",
    "label_count    = np.bincount(np.int64(y_test))\n",
    "\n",
    "print (\"[test]  unique labels:      \", unique_labels)\n",
    "print (\"[test]  count distribution: \", label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c3380-a84a-41fe-85b3-c11f41f57db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "267f9729-f56f-4c44-9c2c-fb26e591647a",
   "metadata": {},
   "source": [
    "### Neural Network Design Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e9aae89-750c-4ba6-94e9-4b75da69d5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:         32\n",
      "max_iter_sgd:       15000\n",
      "num features:       1024\n",
      "hiden layers size:  [30, 5]\n",
      "number of layers:   2\n",
      "number of classes:  5\n",
      "size of training:   10000\n",
      "size of test:       1000\n",
      "isLeastSquare:      0\n"
     ]
    }
   ],
   "source": [
    "learning_rate         = 0.01\n",
    "max_iter              = 15000                        # number of iterations of sgd\n",
    "enable_adaptive_learn = 1\n",
    "batch_size            = 32\n",
    "initial_num_features  = len(x_train[0])\n",
    "#hidden_layer_list     = [512, 256, 128, 64]     # for sigmoid; 50% single layer to 70% many layers\n",
    "hidden_layer_list     =  [30]                    # for reLu; even a single layer gives 70% accuracy; adding more layers did not improve the accuracy\n",
    "#hidden_layer_list     = [50, 40, 30, 20]\n",
    "#hidden_layer_list     = [2, 2, 2, 2]\n",
    "num_classes           = 5\n",
    "hidden_layer_list.append(num_classes)\n",
    "num_layers            = len(hidden_layer_list)\n",
    "train_size            = len(x_train)\n",
    "test_size             = len(x_test)\n",
    "isLeastSquare         = 0                           # (1 - correspoding_output_probability)^2   if 0 then it is log likelyhood\n",
    "isReLu                = 1                           # max(0,z);  if 0 then it is sigmoid (only for hidden layers)\n",
    "\n",
    "if isReLu:                                          # setting up the initial starting point\n",
    "    left_range  = 0                                 \n",
    "    right_range = 1\n",
    "else:\n",
    "    left_range  = -1\n",
    "    right_range = 1\n",
    "\n",
    "print (\"batch size:        \", batch_size)\n",
    "print (\"max_iter_sgd:      \", max_iter)\n",
    "print (\"num features:      \", initial_num_features)\n",
    "print (\"hiden layers size: \", hidden_layer_list)\n",
    "print (\"number of layers:  \", num_layers)\n",
    "print (\"number of classes: \", num_classes)\n",
    "print (\"size of training:  \", train_size)\n",
    "print (\"size of test:      \", test_size)\n",
    "print (\"isLeastSquare:     \", isLeastSquare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f0f8c8-07f0-406f-aa80-88558b83cf0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67147038-2d3f-4cac-9d20-d2ac57dcba86",
   "metadata": {},
   "source": [
    "### Initialize Perceptron Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a6481e-996c-4033-961c-01ab2c5b57d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameter matricies:  2\n"
     ]
    }
   ],
   "source": [
    "row_length            = initial_num_features\n",
    "list_parameter_matrix = []                                      # at index i, stores the parameter matrix for corresponding hidden layer\n",
    "\n",
    "for layer in range(0, len(hidden_layer_list)):        # traversing the hidden layer from left to right\n",
    "    num_rows    = hidden_layer_list[layer]\n",
    "    para_matrix = np.zeros((num_rows, row_length))    # all parameters initialized to 0.5 initially\n",
    "    para_matrix = np.random.uniform(left_range,right_range,(num_rows, row_length))     # for sigmoid, better bw -1 & 1; for relu anything but better with 0 & 1 \n",
    "    #para_matrix.fill(0.2) \n",
    "    list_parameter_matrix.append(para_matrix)\n",
    "    row_length  = num_rows                            # for the next hidden layer\n",
    "\n",
    "print (\"number of parameter matricies: \", len(list_parameter_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328f7c0-213a-4f06-b1c1-92c372534057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "662fed10-45c3-4705-b3d0-c93b7c0ec739",
   "metadata": {},
   "source": [
    "### Computing Output Probabilities Every Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17573ef9-b3e8-48cb-ae47-831551ce308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "e = math.e\n",
    "\n",
    "def softmax_best_pr (output_values, pr, class_label, example_index, isTrain):            # for the final probability\n",
    "    class_label[0] = output_values.argmax() + 1\n",
    "    scaled_values  = output_values - max(output_values)          # to avoid overflow\n",
    "    train_inp      = pow(e, scaled_values)                       # input to the last single softmax-perceptron\n",
    "    sum_values     = train_inp.sum()\n",
    "    if isTrain:\n",
    "        pr[0]          = train_inp[int(y_train[example_index])-1]/sum_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb7a7e76-7050-481e-b6b6-675f8dca101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(train_acc, error, distribution): \n",
    "    train_acc[0]    = 0\n",
    "    error[0]        = 0\n",
    "\n",
    "    for index in range(start_range[0], end_range[0]):\n",
    "        output       = [None]*num_layers\n",
    "        train_inp    = x_train[index]           # row of attributes\n",
    "        for layer in range(0, num_layers):\n",
    "            output[layer]  = np.matmul(list_parameter_matrix[layer],  train_inp.transpose())\n",
    "            for pindex in range(0, np.shape(output[layer])[0]):\n",
    "                \n",
    "                if isReLu:                                             # reLu activation function\n",
    "                    output[layer][pindex]  = max(0, output[layer][pindex])\n",
    "                else:                                                  # sigmoid activation function\n",
    "                    if (output[layer][pindex] > 20):                   # to avoid numerical warnings/errors\n",
    "                        output[layer][pindex]  = 1\n",
    "                    elif (output[layer][pindex] < -20):\n",
    "                        output[layer][pindex]  = 0\n",
    "                    else:\n",
    "                        output[layer][pindex]  = 1/(1+pow(e, -1*output[layer][pindex]))                                       # using sigmoid g(z)\n",
    "            train_inp      = copy.deepcopy(output[layer])                                         # fetch as input to next layer\n",
    "        \n",
    "        final_pr         = [0]\n",
    "        predicted_label  = [0]\n",
    "        softmax_best_pr (output[num_layers-1], final_pr, predicted_label, index, 1)    # isTrain = 1\n",
    "        if predicted_label[0] == y_train[index]:\n",
    "            train_acc[0] += 1\n",
    "            distribution[y_train[index] -1] += 1\n",
    "        \n",
    "        if isLeastSquare:                                      # least squares  =========\n",
    "            error[0] += (1-final_pr[0])*(1-final_pr[0])\n",
    "        else:                                                  # log likelyhood =========\n",
    "            if final_pr[0] != 0:                            \n",
    "                error[0] += -1*math.log(final_pr[0])               \n",
    "            else:\n",
    "                error[0] += 999999\n",
    "        output_list.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d12ebb55-bb67-4dbd-83dc-fe82d59dc8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_full_accuracy (full_acc, distribution, y_pred, isTrain):     # computes the full accuracy of the training and test datas sets\n",
    "    full_acc[0]    = 0\n",
    "    \n",
    "    if (isTrain):\n",
    "        num_samples = len(x_train)\n",
    "    else:\n",
    "        num_samples = len(x_test)\n",
    "        \n",
    "    for index in range(0, num_samples):\n",
    "        output       = [None]*num_layers\n",
    "        \n",
    "        if isTrain:\n",
    "            train_inp    = x_train[index]           # row of attributes\n",
    "        else:\n",
    "            train_inp    = x_test[index]            # row of attributes\n",
    "            \n",
    "        for layer in range(0, num_layers):\n",
    "            output[layer]  = np.matmul(list_parameter_matrix[layer],  train_inp.transpose())\n",
    "            for pindex in range(0, np.shape(output[layer])[0]):\n",
    "                if isReLu:                                             # reLu activation function\n",
    "                    output[layer][pindex]  = max(0, output[layer][pindex])\n",
    "                else:                                                  # sigmoid activation function\n",
    "                    if (output[layer][pindex] > 20):                   # to avoid numerical warnings/errors\n",
    "                        output[layer][pindex]  = 1\n",
    "                    elif (output[layer][pindex] < -20):\n",
    "                        output[layer][pindex]  = 0\n",
    "                    else:\n",
    "                        output[layer][pindex]  = 1/(1+pow(e, -1*output[layer][pindex]))        # using sigmoid g(z)\n",
    "            train_inp      = copy.deepcopy(output[layer])                                  # fetch as input to next layer\n",
    "        \n",
    "        final_pr         = [0]\n",
    "        predicted_label  = [0]\n",
    "        softmax_best_pr (output[num_layers-1], final_pr, predicted_label, index, isTrain)\n",
    "        y_pred[index] = predicted_label\n",
    "        \n",
    "        if isTrain:\n",
    "            if predicted_label[0] == y_train[index]:\n",
    "                full_acc[0] += 1\n",
    "                distribution[y_train[index] -1] += 1\n",
    "        else:\n",
    "            if predicted_label[0] == y_test[index]:\n",
    "                full_acc[0] += 1\n",
    "                distribution[y_test[index] -1] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901f6103-b208-4c14-9f51-6a98b8c3e423",
   "metadata": {},
   "source": [
    "### Computing Gradient: Simple Backpropagation\n",
    "\n",
    "##### each hidden layer:   0-1   sigmoid\n",
    "##### last layer:                 multi sigmoid (softmax)\n",
    "##### loss function:          cross-entropy (basically log likelyhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "684b5f7e-200e-45f8-a28a-a1df6d6fb952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter size for each perceptron of hidden layer:  [1024, 30, 5]\n"
     ]
    }
   ],
   "source": [
    "num_para_hidLay_list  = [initial_num_features] + hidden_layer_list\n",
    "print (\"parameter size for each perceptron of hidden layer: \", num_para_hidLay_list)\n",
    "\n",
    "def update_parameters():\n",
    "    old_para_matrix       = copy.deepcopy(list_parameter_matrix)      ## store the old parameters in a separate varaible\n",
    "    grad_para_x           = copy.deepcopy(list_parameter_matrix)      ## grad x     ### dummy values  ## will be computed further\n",
    "    grad_para_t           = copy.deepcopy(list_parameter_matrix)      ## grad theta ### dummy values  ## will be computed further\n",
    "\n",
    "    for example_index in range(start_range[0], end_range[0]):\n",
    "        for layer in range(num_layers-1, -1, -1):                # traverse the layers in reverse order\n",
    "            if layer+1 == num_layers:\n",
    "                scaled_outputs  = output_list[example_index][layer] - max(output_list[example_index][layer])          # to avoid overflow\n",
    "                train_inp      = pow(e, scaled_outputs)                       # input to the last single softmax-perceptron\n",
    "                sum_values     = train_inp.sum()\n",
    "                sum_para_ary   = train_inp/sum_values\n",
    "                pr_selected    = sum_para_ary[int(y_train[example_index])-1]\n",
    "                \n",
    "                if isLeastSquare:                                         ## === derivative based on least squares  ===\n",
    "                    sum_para_ary = 2*(1-pr_selected)*-1*-1*pr_selected*sum_para_ary\n",
    "                    sum_para_ary[int(y_train[example_index])-1] = 2*(1-pr_selected)*-1*pr_selected*(1-pr_selected)\n",
    "                    \n",
    "                else:                                                     ## === derivative based on log likelyhood ===\n",
    "                    sum_para_ary[int(y_train[example_index])-1] -= 1\n",
    "            else:\n",
    "                sum_para_ary = grad_para_x[layer+1].sum(axis=0)\n",
    "            \n",
    "            if isReLu:                                             # derivative of reLu activation function\n",
    "                #grad_gz        = (output_list[example_index][layer] >= 0).astype(int)       # not enough due to vanashing gradients\n",
    "                grad_gz         = np.where(output_list[example_index][layer] > 0, 1, np.random.rand(1))   # assigning small gradient to negative values\n",
    "                \n",
    "            else:                                                  # derivative of sigmoid activation function\n",
    "                grad_gz        = 1 - output_list[example_index][layer]\n",
    "                grad_gz        = np.multiply(grad_gz, output_list[example_index][layer])\n",
    "                \n",
    "            perceptron_ary = np.multiply(grad_gz, sum_para_ary)\n",
    "            if layer == 0:                                               # [None,:] makes shape (,5)  --->  (1,5)\n",
    "                grad_para_t[layer]        =  np.matmul(perceptron_ary[None,:].transpose(), x_train[example_index][None,:])\n",
    "            else:\n",
    "                grad_para_t[layer]        =  np.matmul(perceptron_ary[None,:].transpose(), output_list[example_index][layer-1][None,:]) \n",
    "            list_parameter_matrix[layer] -=  grad_para_t[layer]*learning_rate\n",
    "            \n",
    "            grad_para_x[layer]            =  old_para_matrix[layer]*perceptron_ary[None,:].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae3557a-ca20-483d-91ff-bc1e44786d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5d0b8df-8474-4d0e-ac11-c6da2ad06ffa",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7570c4b7-53fa-4a00-9121-5081b0ee83f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num sgd iter  :  0\n",
      "train accuracy:  1\n",
      "dist          :  {0: 1, 1: 0, 2: 0, 3: 0, 4: 0}\n",
      "error value   :  51.50201319789124\n",
      "range:           0 32\n",
      "============== new learning rate:  0.0070710678118654745\n",
      "full accuracy :  2291\n",
      "dist          :  {0: 1971, 1: 0, 2: 0, 3: 0, 4: 320}\n",
      "max train accuracy :  2291\n",
      "============== new learning rate:  0.005773502691896258\n",
      "full accuracy :  2257\n",
      "dist          :  {0: 1971, 1: 0, 2: 0, 3: 0, 4: 286}\n",
      "max train accuracy :  2291\n",
      "============== new learning rate:  0.005\n",
      "full accuracy :  2154\n",
      "dist          :  {0: 1971, 1: 0, 2: 0, 3: 0, 4: 183}\n",
      "max train accuracy :  2291\n",
      "\n",
      "num sgd iter  :  1000\n",
      "train accuracy:  12\n",
      "dist          :  {0: 12, 1: 0, 2: 0, 3: 0, 4: 0}\n",
      "error value   :  52.14029993269414\n",
      "range:           2048 2080\n",
      "============== new learning rate:  0.004472135954999579\n",
      "full accuracy :  2309\n",
      "dist          :  {0: 1971, 1: 0, 2: 0, 3: 0, 4: 338}\n",
      "max train accuracy :  2309\n",
      "============== new learning rate:  0.004082482904638631\n",
      "full accuracy :  2321\n",
      "dist          :  {0: 1971, 1: 0, 2: 0, 3: 0, 4: 350}\n",
      "max train accuracy :  2321\n",
      "============== new learning rate:  0.003779644730092272\n",
      "full accuracy :  2226\n",
      "dist          :  {0: 1940, 1: 0, 2: 0, 3: 52, 4: 234}\n",
      "max train accuracy :  2321\n",
      "\n",
      "num sgd iter  :  2000\n",
      "train accuracy:  7\n",
      "dist          :  {0: 7, 1: 0, 2: 0, 3: 0, 4: 0}\n",
      "error value   :  46.44584202592766\n",
      "range:           4096 4128\n",
      "============== new learning rate:  0.0035355339059327372\n",
      "full accuracy :  2453\n",
      "dist          :  {0: 1971, 1: 0, 2: 0, 3: 1, 4: 481}\n",
      "max train accuracy :  2453\n",
      "============== new learning rate:  0.0033333333333333335\n",
      "full accuracy :  2690\n",
      "dist          :  {0: 1971, 1: 0, 2: 0, 3: 0, 4: 719}\n",
      "max train accuracy :  2690\n",
      "============== new learning rate:  0.0031622776601683794\n",
      "full accuracy :  2759\n",
      "dist          :  {0: 1959, 1: 1, 2: 0, 3: 42, 4: 757}\n",
      "max train accuracy :  2759\n",
      "\n",
      "num sgd iter  :  3000\n",
      "train accuracy:  13\n",
      "dist          :  {0: 3, 1: 3, 2: 0, 3: 1, 4: 6}\n",
      "error value   :  43.06216963088653\n",
      "range:           6144 6176\n",
      "============== new learning rate:  0.0030151134457776364\n",
      "full accuracy :  3066\n",
      "dist          :  {0: 1971, 1: 0, 2: 0, 3: 1, 4: 1094}\n",
      "max train accuracy :  3066\n",
      "============== new learning rate:  0.002886751345948129\n",
      "full accuracy :  4375\n",
      "dist          :  {0: 1958, 1: 54, 2: 176, 3: 951, 4: 1236}\n",
      "max train accuracy :  4375\n",
      "============== new learning rate:  0.002773500981126146\n",
      "full accuracy :  4955\n",
      "dist          :  {0: 1954, 1: 302, 2: 261, 3: 1353, 4: 1085}\n",
      "max train accuracy :  4955\n",
      "\n",
      "num sgd iter  :  4000\n",
      "train accuracy:  20\n",
      "dist          :  {0: 5, 1: 6, 2: 2, 3: 0, 4: 7}\n",
      "error value   :  31.578359680817734\n",
      "range:           8192 8224\n",
      "============== new learning rate:  0.002672612419124244\n",
      "full accuracy :  4890\n",
      "dist          :  {0: 1955, 1: 556, 2: 260, 3: 116, 4: 2003}\n",
      "max train accuracy :  4955\n",
      "============== new learning rate:  0.0025819888974716113\n",
      "full accuracy :  5066\n",
      "dist          :  {0: 1954, 1: 585, 2: 405, 3: 97, 4: 2025}\n",
      "max train accuracy :  5066\n",
      "============== new learning rate:  0.0025\n",
      "full accuracy :  5246\n",
      "dist          :  {0: 1957, 1: 717, 2: 468, 3: 105, 4: 1999}\n",
      "max train accuracy :  5246\n",
      "============== new learning rate:  0.00242535625036333\n",
      "full accuracy :  5341\n",
      "dist          :  {0: 1958, 1: 733, 2: 504, 3: 163, 4: 1983}\n",
      "max train accuracy :  5341\n",
      "\n",
      "num sgd iter  :  5000\n",
      "train accuracy:  18\n",
      "dist          :  {0: 3, 1: 7, 2: 2, 3: 1, 4: 5}\n",
      "error value   :  29.764036862986195\n",
      "range:           256 288\n",
      "============== new learning rate:  0.0023570226039551587\n",
      "full accuracy :  5361\n",
      "dist          :  {0: 1956, 1: 778, 2: 495, 3: 112, 4: 2020}\n",
      "max train accuracy :  5361\n",
      "============== new learning rate:  0.0022941573387056176\n",
      "full accuracy :  5513\n",
      "dist          :  {0: 1957, 1: 797, 2: 591, 3: 184, 4: 1984}\n",
      "max train accuracy :  5513\n",
      "============== new learning rate:  0.0022360679774997894\n",
      "full accuracy :  5491\n",
      "dist          :  {0: 1959, 1: 852, 2: 541, 3: 198, 4: 1941}\n",
      "max train accuracy :  5513\n",
      "\n",
      "num sgd iter  :  6000\n",
      "train accuracy:  11\n",
      "dist          :  {0: 1, 1: 1, 2: 1, 3: 7, 4: 1}\n",
      "error value   :  55.83853152780836\n",
      "range:           2304 2336\n",
      "============== new learning rate:  0.002182178902359924\n",
      "full accuracy :  5524\n",
      "dist          :  {0: 1954, 1: 906, 2: 520, 3: 179, 4: 1965}\n",
      "max train accuracy :  5524\n",
      "============== new learning rate:  0.0021320071635561044\n",
      "full accuracy :  5596\n",
      "dist          :  {0: 1959, 1: 878, 2: 613, 3: 240, 4: 1906}\n",
      "max train accuracy :  5596\n",
      "============== new learning rate:  0.002085144140570748\n",
      "full accuracy :  5713\n",
      "dist          :  {0: 1951, 1: 937, 2: 650, 3: 261, 4: 1914}\n",
      "max train accuracy :  5713\n",
      "\n",
      "num sgd iter  :  7000\n",
      "train accuracy:  19\n",
      "dist          :  {0: 6, 1: 0, 2: 2, 3: 0, 4: 11}\n",
      "error value   :  59.56872837300413\n",
      "range:           4352 4384\n",
      "============== new learning rate:  0.0020412414523193153\n",
      "full accuracy :  5764\n",
      "dist          :  {0: 1947, 1: 993, 2: 651, 3: 221, 4: 1952}\n",
      "max train accuracy :  5764\n",
      "============== new learning rate:  0.002\n",
      "full accuracy :  5783\n",
      "dist          :  {0: 1950, 1: 973, 2: 668, 3: 253, 4: 1939}\n",
      "max train accuracy :  5783\n",
      "============== new learning rate:  0.0019611613513818406\n",
      "full accuracy :  5861\n",
      "dist          :  {0: 1949, 1: 1002, 2: 677, 3: 341, 4: 1892}\n",
      "max train accuracy :  5861\n",
      "\n",
      "num sgd iter  :  8000\n",
      "train accuracy:  18\n",
      "dist          :  {0: 3, 1: 3, 2: 6, 3: 5, 4: 1}\n",
      "error value   :  28.969651379782544\n",
      "range:           6400 6432\n",
      "============== new learning rate:  0.0019245008972987524\n",
      "full accuracy :  5938\n",
      "dist          :  {0: 1946, 1: 1003, 2: 725, 3: 362, 4: 1902}\n",
      "max train accuracy :  5938\n",
      "============== new learning rate:  0.001889822365046136\n",
      "full accuracy :  5876\n",
      "dist          :  {0: 1949, 1: 972, 2: 742, 3: 256, 4: 1957}\n",
      "max train accuracy :  5938\n",
      "============== new learning rate:  0.0018569533817705188\n",
      "full accuracy :  5943\n",
      "dist          :  {0: 1944, 1: 1021, 2: 745, 3: 299, 4: 1934}\n",
      "max train accuracy :  5943\n",
      "\n",
      "num sgd iter  :  9000\n",
      "train accuracy:  20\n",
      "dist          :  {0: 9, 1: 1, 2: 4, 3: 2, 4: 4}\n",
      "error value   :  25.844872539078153\n",
      "range:           8448 8480\n",
      "============== new learning rate:  0.0018257418583505537\n",
      "full accuracy :  5905\n",
      "dist          :  {0: 1947, 1: 1008, 2: 698, 3: 272, 4: 1980}\n",
      "max train accuracy :  5943\n",
      "============== new learning rate:  0.0017960530202677492\n",
      "full accuracy :  6103\n",
      "dist          :  {0: 1930, 1: 1130, 2: 732, 3: 393, 4: 1918}\n",
      "max train accuracy :  6103\n",
      "============== new learning rate:  0.0017677669529663686\n",
      "full accuracy :  6111\n",
      "dist          :  {0: 1931, 1: 1114, 2: 739, 3: 393, 4: 1934}\n",
      "max train accuracy :  6111\n",
      "============== new learning rate:  0.0017407765595569785\n",
      "full accuracy :  6114\n",
      "dist          :  {0: 1931, 1: 1110, 2: 739, 3: 403, 4: 1931}\n",
      "max train accuracy :  6114\n",
      "\n",
      "num sgd iter  :  10000\n",
      "train accuracy:  23\n",
      "dist          :  {0: 10, 1: 4, 2: 2, 3: 1, 4: 6}\n",
      "error value   :  23.77845263938844\n",
      "range:           512 544\n",
      "============== new learning rate:  0.0017149858514250882\n",
      "full accuracy :  6168\n",
      "dist          :  {0: 1928, 1: 1147, 2: 752, 3: 428, 4: 1913}\n",
      "max train accuracy :  6168\n",
      "============== new learning rate:  0.0016903085094570332\n",
      "full accuracy :  6230\n",
      "dist          :  {0: 1921, 1: 1188, 2: 746, 3: 495, 4: 1880}\n",
      "max train accuracy :  6230\n",
      "============== new learning rate:  0.0016666666666666668\n",
      "full accuracy :  6210\n",
      "dist          :  {0: 1920, 1: 1171, 2: 750, 3: 445, 4: 1924}\n",
      "max train accuracy :  6230\n",
      "\n",
      "num sgd iter  :  11000\n",
      "train accuracy:  22\n",
      "dist          :  {0: 5, 1: 5, 2: 3, 3: 2, 4: 7}\n",
      "error value   :  26.211849518752643\n",
      "range:           2560 2592\n",
      "============== new learning rate:  0.001643989873053573\n",
      "full accuracy :  6255\n",
      "dist          :  {0: 1913, 1: 1219, 2: 742, 3: 469, 4: 1912}\n",
      "max train accuracy :  6255\n",
      "============== new learning rate:  0.0016222142113076256\n",
      "full accuracy :  6272\n",
      "dist          :  {0: 1917, 1: 1207, 2: 755, 3: 490, 4: 1903}\n",
      "max train accuracy :  6272\n",
      "============== new learning rate:  0.0016012815380508714\n",
      "full accuracy :  6264\n",
      "dist          :  {0: 1915, 1: 1220, 2: 751, 3: 456, 4: 1922}\n",
      "max train accuracy :  6272\n",
      "\n",
      "num sgd iter  :  12000\n",
      "train accuracy:  22\n",
      "dist          :  {0: 10, 1: 5, 2: 2, 3: 0, 4: 5}\n",
      "error value   :  22.67726657600956\n",
      "range:           4608 4640\n",
      "============== new learning rate:  0.0015811388300841897\n",
      "full accuracy :  6268\n",
      "dist          :  {0: 1919, 1: 1205, 2: 772, 3: 449, 4: 1923}\n",
      "max train accuracy :  6272\n",
      "============== new learning rate:  0.0015617376188860608\n",
      "full accuracy :  6309\n",
      "dist          :  {0: 1904, 1: 1226, 2: 776, 3: 487, 4: 1916}\n",
      "max train accuracy :  6309\n",
      "============== new learning rate:  0.0015430334996209192\n",
      "full accuracy :  6304\n",
      "dist          :  {0: 1918, 1: 1210, 2: 760, 3: 509, 4: 1907}\n",
      "max train accuracy :  6309\n",
      "\n",
      "num sgd iter  :  13000\n",
      "train accuracy:  16\n",
      "dist          :  {0: 4, 1: 3, 2: 4, 3: 2, 4: 3}\n",
      "error value   :  29.383501879986046\n",
      "range:           6656 6688\n",
      "============== new learning rate:  0.0015249857033260467\n",
      "full accuracy :  6327\n",
      "dist          :  {0: 1909, 1: 1229, 2: 765, 3: 514, 4: 1910}\n",
      "max train accuracy :  6327\n",
      "============== new learning rate:  0.0015075567228888182\n",
      "full accuracy :  6340\n",
      "dist          :  {0: 1901, 1: 1249, 2: 774, 3: 502, 4: 1914}\n",
      "max train accuracy :  6340\n",
      "============== new learning rate:  0.0014907119849998597\n",
      "full accuracy :  6346\n",
      "dist          :  {0: 1899, 1: 1259, 2: 776, 3: 493, 4: 1919}\n",
      "max train accuracy :  6346\n",
      "\n",
      "num sgd iter  :  14000\n",
      "train accuracy:  19\n",
      "dist          :  {0: 3, 1: 5, 2: 5, 3: 2, 4: 4}\n",
      "error value   :  33.02203234871545\n",
      "range:           8704 8736\n",
      "============== new learning rate:  0.0014744195615489714\n",
      "full accuracy :  6361\n",
      "dist          :  {0: 1896, 1: 1287, 2: 758, 3: 501, 4: 1919}\n",
      "max train accuracy :  6361\n",
      "============== new learning rate:  0.0014586499149789457\n",
      "full accuracy :  6382\n",
      "dist          :  {0: 1898, 1: 1265, 2: 782, 3: 517, 4: 1920}\n",
      "max train accuracy :  6382\n",
      "============== new learning rate:  0.0014433756729740645\n",
      "full accuracy :  6388\n",
      "dist          :  {0: 1900, 1: 1260, 2: 797, 3: 520, 4: 1911}\n",
      "max train accuracy :  6388\n",
      "============== new learning rate:  0.0014285714285714286\n",
      "full accuracy :  6382\n",
      "dist          :  {0: 1899, 1: 1270, 2: 782, 3: 512, 4: 1919}\n",
      "max train accuracy :  6388\n"
     ]
    }
   ],
   "source": [
    "flag = 0\n",
    "direction      = 1\n",
    "num_iter       = 0\n",
    "error          = [1]\n",
    "start_range    = [0]\n",
    "end_range      = [0]\n",
    "train_acc      = [0]\n",
    "full_acc       = [0]\n",
    "max_acc        = 0\n",
    "distribution   = {0:0,1:0,2:0,3:0,4:0}\n",
    "num_epochs     = 1\n",
    "seed_learning_rate = learning_rate\n",
    "best_parameter_matrix = copy.deepcopy(list_parameter_matrix)\n",
    "\n",
    "while (error[0] > 0.00001) and num_iter < max_iter:\n",
    "    if start_range[0] == 0:\n",
    "        output_list  = []                  # output[index] stores the list of outputs for training example_index <index> \n",
    "    end_range[0] = start_range[0] + batch_size \n",
    "    \n",
    "    distribution = {0:0,1:0,2:0,3:0,4:0}\n",
    "    compute_error(train_acc, error, distribution)        # compute the log likelyhood error/cost fn of sum of all examples/data-points\n",
    "    \n",
    "    if num_iter%1000 == 0:\n",
    "        print (\"\\nnum sgd iter  : \", num_iter)\n",
    "        print (\"train accuracy: \", train_acc[0])\n",
    "        print (\"dist          : \", distribution)\n",
    "        print (\"error value   : \", error[0])\n",
    "        print (\"range:          \", start_range[0], end_range[0])\n",
    "\n",
    "    update_parameters()                # update the parameters of each perceptron using gradient descent\n",
    "    num_iter +=1;\n",
    "\n",
    "    start_range[0] = start_range[0] + batch_size\n",
    "    if (start_range[0] >= 10000-33):\n",
    "        start_range[0] = 0\n",
    "        num_epochs += 1\n",
    "        if enable_adaptive_learn:\n",
    "            learning_rate = seed_learning_rate/math.sqrt(num_epochs)    # it is called inverse scaling\n",
    "            print (\"============== new learning rate: \", learning_rate)\n",
    "        \n",
    "        ###### used for debugging as well ; to see regular change in accuracy; change learning rate accordingly ###### \n",
    "        distribution = {0:0,1:0,2:0,3:0,4:0}\n",
    "        y_pred       = [None] * len(x_train)\n",
    "        compute_full_accuracy(full_acc, distribution, y_pred, 1)   # isTrain = 1\n",
    "        print (\"full accuracy : \", full_acc[0])\n",
    "        print (\"dist          : \", distribution)\n",
    "        if (max_acc < full_acc[0]):\n",
    "            max_acc = full_acc[0]\n",
    "            best_parameter_matrix = copy.deepcopy(list_parameter_matrix)\n",
    "        print (\"max train accuracy : \", max_acc)\n",
    "        \n",
    "list_parameter_matrix = copy.deepcopy(best_parameter_matrix)    # choose the parameters that gave the best train accuarcy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151fc7ba-ca0f-43da-bba9-2561c6f51d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09fb543a-6908-4014-8910-97ec4d873351",
   "metadata": {},
   "source": [
    "### Train & Test Accuracies & PR Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b27d391-bc7b-4d68-8607-e316f635fade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full train accuracy:  6388\n",
      "full train accuracy:  0.6388\n",
      "dist               :  {0: 1900, 1: 1260, 2: 797, 3: 520, 4: 1911}\n",
      "PR Report          : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.96      0.86      1971\n",
      "           2       0.64      0.64      0.64      1978\n",
      "           3       0.59      0.41      0.48      1952\n",
      "           4       0.49      0.26      0.34      2008\n",
      "           5       0.60      0.91      0.72      2091\n",
      "\n",
      "    accuracy                           0.64     10000\n",
      "   macro avg       0.62      0.64      0.61     10000\n",
      "weighted avg       0.62      0.64      0.61     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "distribution = {0:0,1:0,2:0,3:0,4:0}\n",
    "y_pred       = [None] * len(x_train)\n",
    "compute_full_accuracy(full_acc, distribution, y_pred, 1)\n",
    "print (\"full train accuracy: \", full_acc[0])\n",
    "print (\"full train accuracy: \", full_acc[0]/len(x_train))\n",
    "print (\"dist               : \", distribution)\n",
    "print(\"PR Report          : \\n\", classification_report(y_train, y_pred, labels=[1, 2, 3, 4, 5], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07882b0f-9107-48df-90a7-9c339b37a65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full test accuracy:  623\n",
      "full test accuracy:  0.623\n",
      "dist              :  {0: 205, 1: 115, 2: 76, 3: 56, 4: 171}\n",
      "PR Report         : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.96      0.87       214\n",
      "           2       0.56      0.63      0.59       183\n",
      "           3       0.65      0.35      0.45       219\n",
      "           4       0.48      0.29      0.36       191\n",
      "           5       0.56      0.89      0.69       193\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.62      0.59      1000\n",
      "weighted avg       0.62      0.62      0.60      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distribution = {0:0,1:0,2:0,3:0,4:0}\n",
    "y_pred       = [None] * len(y_test)\n",
    "compute_full_accuracy(full_acc, distribution, y_pred, 0)\n",
    "print (\"full test accuracy: \", full_acc[0])\n",
    "print (\"full test accuracy: \", full_acc[0]/len(x_test))\n",
    "print (\"dist              : \", distribution)\n",
    "print(\"PR Report         : \\n\", classification_report(y_test, y_pred, labels=[1, 2, 3, 4, 5], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761236af-9b32-4714-b5f5-776865e6821a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ff685-91aa-4ec2-99e3-714c1bfe7f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a2b3bc-586d-49dc-a60d-abc2c64ad4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0331143-d074-4c9e-9d85-0ae259e60f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15441d18-f705-455f-8328-6455bd294f09",
   "metadata": {},
   "source": [
    "##### JUNK: Explanation of Matrix Multiplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5f4407e-2bbd-43ff-b072-06c73b9eec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#             for perceptron in range(0, hidden_layer_list[layer]):\n",
    "#                     y_out                 = output_list[example_index][layer][perceptron]\n",
    "#                     if layer == 0:\n",
    "#                         local_grad_t      = y_out*(1-y_out)*x_train[example_index]\n",
    "#                     else:\n",
    "#                         local_grad_t      = y_out*(1-y_out)*output_list[example_index][layer-1]\n",
    "                        \n",
    "#                     grad_para_t[layer][perceptron]             =  local_grad_t*sum_para_ary[perceptron]\n",
    "#                     list_parameter_matrix[layer][perceptron]  -=  grad_para_t[layer][perceptron]*learning_rate\n",
    "                    \n",
    "#                     local_grad_x                               =  y_out*(1-y_out)*old_para_matrix[layer][perceptron]\n",
    "#                     grad_para_x[layer][perceptron]             =  local_grad_x*sum_para_ary[perceptron]\n",
    "                    \n",
    "#                    sum_para_ary           = grad_para_x[layer+1][:,para_index_forward_layer].sum()  # column vector read as a row vector\n",
    "                \n",
    "#                 for para_index in range(0, num_para_hidLay_list[layer]):\n",
    "#                     y_out                                     = output_list[example_index][layer][perceptron]\n",
    "#                     if layer == 0:\n",
    "#                         x_in                                  = x_train[example_index][para_index]\n",
    "#                     else:\n",
    "#                         x_in                                  = output_list[example_index][layer-1][para_index]\n",
    "#                     local_grad_t                              = y_out*(1-y_out)*x_in\n",
    "#                     grad_para_t[layer][perceptron][para_index]             = sum_para_ary*local_grad_t   # multiply each element with local grad\n",
    "#                     list_parameter_matrix[layer][perceptron][para_index]  -= grad_para_t[layer][perceptron][para_index]*learning_rate\n",
    "                    \n",
    "#                     local_grad_x                                           = y_out*(1-y_out)*old_para_matrix[layer][perceptron][para_index]\n",
    "#                     grad_para_x[layer][perceptron][para_index]             = sum_para_ary*local_grad_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c4d6b-96fd-4975-b1a3-7c818148d7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3dbe71-07c9-4818-9d57-3cd86063a22d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
