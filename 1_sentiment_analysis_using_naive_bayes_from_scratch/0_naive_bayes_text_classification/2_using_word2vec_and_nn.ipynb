{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46d0988-7806-4926-a1ee-c0f2ad09533b",
   "metadata": {},
   "source": [
    "## ========= Text Classification using Word2Vec/BERT & Neural Net ==========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a0ed68-e3e1-499f-95bf-461f2d623efa",
   "metadata": {},
   "source": [
    "#### Link: https://www.youtube.com/watch?v=hQwFeIupNP0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6066790f-2886-4736-be91-bd53b7701458",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_lemmatizer_stemmer = 1    # includes stopword removal as well; did not improve the accuracy\n",
    "balance_dataset        = 1    # if some class has less proportion in training dataset, duplicate it sufficient times\n",
    "use_word2vec           = 1    # if 0, then use the bag of words (but kernel died) # if 2, use BERT embeddings\n",
    "\n",
    "# regular scikit-learn naive-bayes accuracy                       = 0.70\n",
    "# regular BOW and MLP (Multi Layer Perceptron) scikit-learn       = kernel died\n",
    "# regular word2vec and MLP (Multi Layer Perceptron) scikit-learn  = 0.69\n",
    "# regular word2vec and MLP (Multi Layer Perceptron) scikit-learn  = 0.63  (smaller embedding used for fast convergence)\n",
    "\n",
    "'''\n",
    "to use BERT, first please create conda env and install all packages\n",
    "Link: https://stackoverflow.com/questions/54843067/no-module-named-torch\n",
    "'''\n",
    "\n",
    "assert (use_lemmatizer_stemmer  == 1 or use_lemmatizer_stemmer  == 0)\n",
    "assert (balance_dataset         == 1 or balance_dataset         == 0)\n",
    "assert (use_word2vec            == 2 or use_word2vec            == 1 or use_word2vec            == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0bb85-0dbc-4745-99ef-5fa47c3998df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "006c1be0-ef0b-467c-9359-2db01d687715",
   "metadata": {},
   "source": [
    "### Reading training data from .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6e1a49b-5258-4c7f-b7b1-6bc8193d002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "__location__ = os.path.realpath(os.path.join(os.getcwd(), \"dataset_corona_sentiment/Corona_train.csv\"))\n",
    "df_train = pd.read_csv(__location__, dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9acc30b8-2501-450f-bb6e-086dfe47aa0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>CoronaTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22979</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I see all kinds of academics already whipping ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9880</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@HenrySmithUK can you raise with Boris please ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35761</td>\n",
       "      <td>Negative</td>\n",
       "      <td>It s a confusing odd time for the shopping pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37968</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Blog Summary: The Impact of COVID-19 on the Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19709</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>??????? ??????? ???\\r\\r\\nWaiting in a long Que...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID Sentiment                                        CoronaTweet\n",
       "0  22979  Positive  I see all kinds of academics already whipping ...\n",
       "1   9880  Negative  @HenrySmithUK can you raise with Boris please ...\n",
       "2  35761  Negative  It s a confusing odd time for the shopping pub...\n",
       "3  37968  Positive  Blog Summary: The Impact of COVID-19 on the Ca...\n",
       "4  19709   Neutral  ??????? ??????? ???\\r\\r\\nWaiting in a long Que..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7388169f-8fbc-4616-89a7-36df895ac291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows    : 37864\n",
      "number of columns : 3\n",
      "column values     : ['ID', 'Sentiment', 'CoronaTweet']\n",
      "\n",
      "distribution of class lebels : {'Positive': 16602, 'Negative': 14166, 'Neutral': 7096}\n",
      "\n",
      "first row item  : {'ID': '22979', 'Sentiment': 'Positive', 'CoronaTweet': 'I see all kinds of academics already whipping up some #Covid_19 related projects, cfp, syllabi, articles, and blog posts.\\r\\r\\n\\r\\r\\nIÂ\\x92m sittin over here browsing all the food left &amp; tryin to figure out when to go back out to the grocery store. Apparently I donÂ\\x92t do well in pandemic'}\n"
     ]
    }
   ],
   "source": [
    "print (\"number of rows    :\", df_train.shape[0])\n",
    "print (\"number of columns :\", df_train.shape[1])\n",
    "print (\"column values     :\", list(df_train.columns.values))\n",
    "\n",
    "print (\"\\ndistribution of class lebels :\", dict(df_train['Sentiment'].value_counts()))\n",
    "print (\"\\nfirst row item  :\", dict(df_train.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4785e4-73b1-42bc-955a-fda41080f81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f0af457-1cd5-4406-ac53-3f69ba46dc35",
   "metadata": {},
   "source": [
    "### Balance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d22da0-152e-4f7e-b653-fce615a24c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "distribution of class lebels : {'Positive': 16602, 'Neutral': 14192, 'Negative': 14166}\n"
     ]
    }
   ],
   "source": [
    "if balance_dataset == 1:\n",
    "    df_train_net = df_train[df_train.Sentiment == 'Neutral']\n",
    "    df_train = pd.concat([df_train, df_train_net])\n",
    "print (\"\\ndistribution of class lebels :\", dict(df_train['Sentiment'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cebf7b-244f-4661-bae9-ea2f05f7bb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a90b3f3-6525-478a-a6d9-102011362c74",
   "metadata": {},
   "source": [
    "### Reading validation data from .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f91a7d-1c88-408f-9a98-a5fe0ea8766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "__location__ = os.path.realpath(os.path.join(os.getcwd(), \"dataset_corona_sentiment/Corona_validation.csv\"))\n",
    "df_valid = pd.read_csv(__location__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c991d966-80ac-436f-a683-aa228fce9889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows    : 3293\n",
      "number of columns : 3\n",
      "column values     : ['ID', 'Sentiment', 'CoronaTweet']\n",
      "\n",
      "distribution of class lebels : {'Positive': 1444, 'Negative': 1232, 'Neutral': 617}\n",
      "\n",
      "first row item  : {'ID': 7184, 'Sentiment': 'Negative', 'CoronaTweet': 'I reflected on my own consumer behaviour last week and made this list\\r\\r\\nI confess - as much as I feel bad for people who may lose jobs due to the COVID-19, part of me also wish that unethical businesses will no longer be able to operate \"as usual\" unless making changes #time4change https://t.co/63lXRFi82N'}\n"
     ]
    }
   ],
   "source": [
    "print (\"number of rows    :\", df_valid.shape[0])\n",
    "print (\"number of columns :\", df_valid.shape[1])\n",
    "print (\"column values     :\", list(df_valid.columns.values))\n",
    "\n",
    "print (\"\\ndistribution of class lebels :\", dict(df_valid['Sentiment'].value_counts()))\n",
    "print (\"\\nfirst row item  :\", dict(df_valid.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "802ddc22-0616-4132-87c1-5539471cb042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>CoronaTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7184</td>\n",
       "      <td>Negative</td>\n",
       "      <td>I reflected on my own consumer behaviour last ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36363</td>\n",
       "      <td>Negative</td>\n",
       "      <td>I know everyone is getting stir crazy but befo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10423</td>\n",
       "      <td>Negative</td>\n",
       "      <td>I haven t seen gas prices this low since I fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6409</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Only batmeat left on the supermarket shelves\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7015</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Along with health workers, we need to apprecia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID Sentiment                                        CoronaTweet\n",
       "0   7184  Negative  I reflected on my own consumer behaviour last ...\n",
       "1  36363  Negative  I know everyone is getting stir crazy but befo...\n",
       "2  10423  Negative  I haven t seen gas prices this low since I fir...\n",
       "3   6409   Neutral  Only batmeat left on the supermarket shelves\\r...\n",
       "4   7015   Neutral  Along with health workers, we need to apprecia..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927d719-e1e6-4ff0-b9be-a627dcc252c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92824fe6-048f-4613-b4c3-58076c567001",
   "metadata": {},
   "source": [
    "### Perform Lematization & Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87e261-8c03-48a0-91f8-ddbfb8945fe5",
   "metadata": {},
   "source": [
    "##### taken from here: https://medium.com/analytics-vidhya/nlp-tutorial-for-text-classification-in-python-8f19cd17b49e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afc249fb-7236-4200-86a5-31734d27c9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dishantgoyal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/dishantgoyal/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/dishantgoyal/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/dishantgoyal/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb26c903-3fb2-49db-b31c-ebb5db159dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()  \n",
    "    text=re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    " \n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    if use_word2vec == 1:\n",
    "        return a\n",
    "    else:\n",
    "        return \" \".join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13f71dcb-20ae-42c2-8051-207917496289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "if use_lemmatizer_stemmer == 1:\n",
    "    df_valid['CleanTweet'] = df_valid['CoronaTweet'].apply(lambda x: finalpreprocess(x))\n",
    "    df_train['CleanTweet'] = df_train['CoronaTweet'].apply(lambda x: finalpreprocess(x))\n",
    "    df_valid.head()\n",
    "    df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67a97111-348e-490a-865f-32b42f77567e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>CoronaTweet</th>\n",
       "      <th>CleanTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22979</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I see all kinds of academics already whipping ...</td>\n",
       "      <td>[see, kind, academic, already, whip, covid, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9880</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@HenrySmithUK can you raise with Boris please ...</td>\n",
       "      <td>[henrysmithuk, raise, boris, please, supermark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35761</td>\n",
       "      <td>Negative</td>\n",
       "      <td>It s a confusing odd time for the shopping pub...</td>\n",
       "      <td>[confuse, odd, time, shop, public, store, clos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37968</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Blog Summary: The Impact of COVID-19 on the Ca...</td>\n",
       "      <td>[blog, summary, impact, covid, canadian, resid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19709</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>??????? ??????? ???\\r\\r\\nWaiting in a long Que...</td>\n",
       "      <td>[wait, long, queue, enter, supermarket, finall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID Sentiment                                        CoronaTweet  \\\n",
       "0  22979  Positive  I see all kinds of academics already whipping ...   \n",
       "1   9880  Negative  @HenrySmithUK can you raise with Boris please ...   \n",
       "2  35761  Negative  It s a confusing odd time for the shopping pub...   \n",
       "3  37968  Positive  Blog Summary: The Impact of COVID-19 on the Ca...   \n",
       "4  19709   Neutral  ??????? ??????? ???\\r\\r\\nWaiting in a long Que...   \n",
       "\n",
       "                                          CleanTweet  \n",
       "0  [see, kind, academic, already, whip, covid, re...  \n",
       "1  [henrysmithuk, raise, boris, please, supermark...  \n",
       "2  [confuse, odd, time, shop, public, store, clos...  \n",
       "3  [blog, summary, impact, covid, canadian, resid...  \n",
       "4  [wait, long, queue, enter, supermarket, finall...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48804bc-bc53-4b54-a1e3-0e233550a7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f8cac05-1a08-422a-9694-c9ff50b41894",
   "metadata": {},
   "source": [
    "### Convert Data to Sentence Embeddings (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25515bc2-257c-4f94-8c03-109e0e1e4de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_word2vec == 1:\n",
    "    from gensim.models import KeyedVectors  # Load pre-trained Word2Vec model (e.g., Google News vectors)\n",
    "    word2vecModel = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "    '''\n",
    "    wget https://figshare.com/ndownloader/files/10798046 -O GoogleNews-vectors-negative300.bin\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fe06cba-c1c2-4f97-a092-df4d110c0563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(tokens):\n",
    "    vectors = [word2vecModel[word] for word in tokens if word in word2vecModel]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)  # Average vector\n",
    "    else:\n",
    "        return [0] * word2vecModel.vector_size  # Zero vector if no words are in the vocabulary\n",
    "\n",
    "if use_word2vec == 1:\n",
    "    df_train['TweetEmbeddings'] = df_train['CleanTweet'].apply(lambda x: get_sentence_embedding(x))\n",
    "    df_valid['TweetEmbeddings'] = df_valid['CleanTweet'].apply(lambda x: get_sentence_embedding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc4ee6ee-d15f-4b33-87ee-10868a11a782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>CoronaTweet</th>\n",
       "      <th>CleanTweet</th>\n",
       "      <th>TweetEmbeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22979</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I see all kinds of academics already whipping ...</td>\n",
       "      <td>[see, kind, academic, already, whip, covid, re...</td>\n",
       "      <td>[0.05482131, 0.030006409, -0.031684287, 0.1578...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9880</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@HenrySmithUK can you raise with Boris please ...</td>\n",
       "      <td>[henrysmithuk, raise, boris, please, supermark...</td>\n",
       "      <td>[0.040020753, 0.0099823, -0.024865722, 0.10838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35761</td>\n",
       "      <td>Negative</td>\n",
       "      <td>It s a confusing odd time for the shopping pub...</td>\n",
       "      <td>[confuse, odd, time, shop, public, store, clos...</td>\n",
       "      <td>[0.0486472, 0.03554862, 0.011557443, 0.1152670...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37968</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Blog Summary: The Impact of COVID-19 on the Ca...</td>\n",
       "      <td>[blog, summary, impact, covid, canadian, resid...</td>\n",
       "      <td>[-0.08162649, -0.04352078, -0.053622596, 0.042...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19709</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>??????? ??????? ???\\r\\r\\nWaiting in a long Que...</td>\n",
       "      <td>[wait, long, queue, enter, supermarket, finall...</td>\n",
       "      <td>[-0.100792825, 0.05029656, -0.027210908, 0.156...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID Sentiment                                        CoronaTweet  \\\n",
       "0  22979  Positive  I see all kinds of academics already whipping ...   \n",
       "1   9880  Negative  @HenrySmithUK can you raise with Boris please ...   \n",
       "2  35761  Negative  It s a confusing odd time for the shopping pub...   \n",
       "3  37968  Positive  Blog Summary: The Impact of COVID-19 on the Ca...   \n",
       "4  19709   Neutral  ??????? ??????? ???\\r\\r\\nWaiting in a long Que...   \n",
       "\n",
       "                                          CleanTweet  \\\n",
       "0  [see, kind, academic, already, whip, covid, re...   \n",
       "1  [henrysmithuk, raise, boris, please, supermark...   \n",
       "2  [confuse, odd, time, shop, public, store, clos...   \n",
       "3  [blog, summary, impact, covid, canadian, resid...   \n",
       "4  [wait, long, queue, enter, supermarket, finall...   \n",
       "\n",
       "                                     TweetEmbeddings  \n",
       "0  [0.05482131, 0.030006409, -0.031684287, 0.1578...  \n",
       "1  [0.040020753, 0.0099823, -0.024865722, 0.10838...  \n",
       "2  [0.0486472, 0.03554862, 0.011557443, 0.1152670...  \n",
       "3  [-0.08162649, -0.04352078, -0.053622596, 0.042...  \n",
       "4  [-0.100792825, 0.05029656, -0.027210908, 0.156...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b2240-b014-42f5-aba8-8ace4d90ea7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c900e4f4-e65c-4760-b288-951d0a71d022",
   "metadata": {},
   "source": [
    "### Convert Data to Sentence Embeddings (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80a06acc-2731-4ac9-a57b-b5c5c68ec687",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_word2vec == 2:   # using DistilBert for faster embedding generation\n",
    "    import torch\n",
    "    from transformers import DistilBertTokenizer, DistilBertModel\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    bertModel = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f460d958-e758-48dc-98a3-30f32c64077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Generate Embeddings\n",
    "def get_bert_embeddings(input_string):\n",
    "    tokens_dict = tokenizer(input_string, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "    with torch.no_grad():\n",
    "        outputs = bertModel(**tokens_dict)\n",
    "    # Extract the last hidden state (CLS token) for sentence embedding\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "    return cls_embedding\n",
    "\n",
    "if use_word2vec == 2:\n",
    "    df_train['TweetEmbeddings'] = df_train['CleanTweet'].apply(lambda x: get_bert_embeddings(x))\n",
    "    df_valid['TweetEmbeddings'] = df_valid['CleanTweet'].apply(lambda x: get_bert_embeddings(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "350d484a-90d9-44de-9968-6f87af9200b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>CoronaTweet</th>\n",
       "      <th>CleanTweet</th>\n",
       "      <th>TweetEmbeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22979</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I see all kinds of academics already whipping ...</td>\n",
       "      <td>[see, kind, academic, already, whip, covid, re...</td>\n",
       "      <td>[0.05482131, 0.030006409, -0.031684287, 0.1578...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9880</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@HenrySmithUK can you raise with Boris please ...</td>\n",
       "      <td>[henrysmithuk, raise, boris, please, supermark...</td>\n",
       "      <td>[0.040020753, 0.0099823, -0.024865722, 0.10838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35761</td>\n",
       "      <td>Negative</td>\n",
       "      <td>It s a confusing odd time for the shopping pub...</td>\n",
       "      <td>[confuse, odd, time, shop, public, store, clos...</td>\n",
       "      <td>[0.0486472, 0.03554862, 0.011557443, 0.1152670...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37968</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Blog Summary: The Impact of COVID-19 on the Ca...</td>\n",
       "      <td>[blog, summary, impact, covid, canadian, resid...</td>\n",
       "      <td>[-0.08162649, -0.04352078, -0.053622596, 0.042...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19709</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>??????? ??????? ???\\r\\r\\nWaiting in a long Que...</td>\n",
       "      <td>[wait, long, queue, enter, supermarket, finall...</td>\n",
       "      <td>[-0.100792825, 0.05029656, -0.027210908, 0.156...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID Sentiment                                        CoronaTweet  \\\n",
       "0  22979  Positive  I see all kinds of academics already whipping ...   \n",
       "1   9880  Negative  @HenrySmithUK can you raise with Boris please ...   \n",
       "2  35761  Negative  It s a confusing odd time for the shopping pub...   \n",
       "3  37968  Positive  Blog Summary: The Impact of COVID-19 on the Ca...   \n",
       "4  19709   Neutral  ??????? ??????? ???\\r\\r\\nWaiting in a long Que...   \n",
       "\n",
       "                                          CleanTweet  \\\n",
       "0  [see, kind, academic, already, whip, covid, re...   \n",
       "1  [henrysmithuk, raise, boris, please, supermark...   \n",
       "2  [confuse, odd, time, shop, public, store, clos...   \n",
       "3  [blog, summary, impact, covid, canadian, resid...   \n",
       "4  [wait, long, queue, enter, supermarket, finall...   \n",
       "\n",
       "                                     TweetEmbeddings  \n",
       "0  [0.05482131, 0.030006409, -0.031684287, 0.1578...  \n",
       "1  [0.040020753, 0.0099823, -0.024865722, 0.10838...  \n",
       "2  [0.0486472, 0.03554862, 0.011557443, 0.1152670...  \n",
       "3  [-0.08162649, -0.04352078, -0.053622596, 0.042...  \n",
       "4  [-0.100792825, 0.05029656, -0.027210908, 0.156...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac726ad1-9415-4738-b719-a6f6cfe2a37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f63a1da7-659e-4545-9d5d-41e2168180c3",
   "metadata": {},
   "source": [
    "### Converting to Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e0c5013-d654-4f97-a797-226ecf26ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_train = np.array(list(df_train['Sentiment']))\n",
    "y_valid = np.array(list(df_valid['Sentiment']))\n",
    "    \n",
    "if use_word2vec >= 1:\n",
    "    x_train_vec = np.array(list(df_train['TweetEmbeddings']))\n",
    "    x_valid_vec = np.array(list(df_valid['TweetEmbeddings']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fa82a3a-25f8-495e-99e1-a6c7e6ace1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_word2vec >= 1:\n",
    "    #print (\"number of data points     :\", len(x_valid))\n",
    "    #print (\"number of class labels    :\", len(y_valid))\n",
    "    assert(len(x_train_vec) == len(y_train))\n",
    "    assert(len(x_valid_vec) == len(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05626ffe-67be-419b-985e-ce72c54f2538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01b4e2c3-491b-40e4-8d36-314e9d703390",
   "metadata": {},
   "source": [
    "### Convert Data to Frequency Vectors (Bag of Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af073def-5545-4420-a354-fce5c1bab2e0",
   "metadata": {},
   "source": [
    "##### fed into scikit learn in this form; numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21f406a2-e814-4e79-9fd5-97924d914e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_word2vec == 0:\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from wordcloud import WordCloud, STOPWORDS\n",
    "    stopwords = list(STOPWORDS)\n",
    "\n",
    "    vectorizer  = CountVectorizer()\n",
    "\n",
    "    x_train = list(df_train['CleanTweet'])\n",
    "    x_valid = list(df_valid['CleanTweet'])\n",
    "    \n",
    "    x_train_vec = (vectorizer.fit_transform(x_train)).toarray()\n",
    "    x_valid_vec = vectorizer.transform(x_valid)                   # uses vocab of training data set # for unseen words freq = 0\n",
    "\n",
    "# x_vec       = (vectorizer.fit_transform(x_train + x_valid)).toarray()\n",
    "# x_train_vec = x_vec[:len(x_train)]\n",
    "# x_valid_vec = x_vec[len(x_train): len(x_train) + len(x_valid)]  \n",
    "# you should not be validation set for training or pre-processing for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cab55bd-f82d-4b47-aef1-1e1de83d2893",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_word2vec == 0:\n",
    "    print (vectorizer.get_feature_names_out()[12345:12355])\n",
    "    print (x_train_vec)\n",
    "    print (stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddcfdcf-92c9-4dd9-a06e-65eda5b29fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffca8b91-7cdf-4ca4-a99b-0cd8e1d8b635",
   "metadata": {},
   "source": [
    "### MLP Classifier Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f6036c5-9e31-4425-8328-bcc5c82004fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dishantgoyal/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(random_state=1, max_iter=300).fit(x_train_vec, y_train)\n",
    "\n",
    "# number of iter = 300 (epochs), relu, adam, learning rate = 0.001, batch size = 'auto', last activation: softmax, \n",
    "# hidden layer = [100], alpha = 0.0001\n",
    "# setting random state ensures reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38723d1c-e1c0-4e91-a677-c184ca68e6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfbb8424-ae8c-4816-aabf-ab1bc078cd88",
   "metadata": {},
   "source": [
    "### Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a853162c-3ab7-43b2-a4c2-b2b6a3f303a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred  = clf.predict(x_valid_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb360b08-2077-4d91-af80-9ced0a6b4caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR Report         : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.72      0.76      0.74      1444\n",
      "    Negative       0.71      0.68      0.69      1232\n",
      "     Neutral       0.58      0.55      0.56       617\n",
      "\n",
      "    accuracy                           0.69      3293\n",
      "   macro avg       0.67      0.66      0.67      3293\n",
      "weighted avg       0.69      0.69      0.69      3293\n",
      "\n",
      "Confusion Matrix  : \n",
      " [[ 838  120  274]\n",
      " [ 125  340  152]\n",
      " [ 218  130 1096]]\n",
      "\n",
      "Accuracy        :  0.6905557242635895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "classes = ['Positive', 'Negative', 'Neutral']\n",
    "print(\"PR Report         : \\n\", classification_report(y_valid, y_valid_pred, labels=classes, zero_division=0))\n",
    "print(\"Confusion Matrix  : \\n\", confusion_matrix(y_valid, y_valid_pred))\n",
    "print(\"\\nAccuracy        : \", accuracy_score(y_valid, y_valid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c4930-2391-441e-95f7-00ef37c269f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494470e-b29d-4389-92fe-d2ab9a658452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
