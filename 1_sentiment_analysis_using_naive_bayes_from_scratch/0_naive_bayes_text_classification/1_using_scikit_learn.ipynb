{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46d0988-7806-4926-a1ee-c0f2ad09533b",
   "metadata": {},
   "source": [
    "## ================== Naive Bayes for Text Classification ============="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f9d5ca-36b5-4136-9f1a-2c629e0912c9",
   "metadata": {},
   "source": [
    "##### Initial vocab consists of both the training and validation words\n",
    "##### Unline our previous implementation, where we only trained using training words & use laplace smoothining for remaining unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6066790f-2886-4736-be91-bd53b7701458",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_bigrams_stopwords  = 0    # could not verify this # however, just removing stopwords had no affect on the accuracy\n",
    "use_word2vec           = 0\n",
    "use_lemmatizer_stemmer = 0    # did not improve the accuracy\n",
    "balance_dataset        = 1    # if some class has less proportion in training dataset, duplicate it sufficient times\n",
    "\n",
    "# regular scikit-learn accuracy = 0.70\n",
    "# + balance_dataset             = 0.71\n",
    "\n",
    "assert (use_bigrams_stopwords   == 1 or use_bigrams_stopwords   == 0)\n",
    "assert (use_lemmatizer_stemmer  == 1 or use_lemmatizer_stemmer  == 0)\n",
    "assert (use_word2vec            == 1 or use_word2vec            == 0)\n",
    "assert (balance_dataset == 1 or balance_dataset == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c1be0-ef0b-467c-9359-2db01d687715",
   "metadata": {},
   "source": [
    "### Reading training data from .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6e1a49b-5258-4c7f-b7b1-6bc8193d002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "__location__ = os.path.realpath(os.path.join(os.getcwd(), \"dataset_corona_sentiment/Corona_train.csv\"))\n",
    "df_train = pd.read_csv(__location__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9acc30b8-2501-450f-bb6e-086dfe47aa0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>CoronaTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22979</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I see all kinds of academics already whipping ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9880</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@HenrySmithUK can you raise with Boris please ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35761</td>\n",
       "      <td>Negative</td>\n",
       "      <td>It s a confusing odd time for the shopping pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37968</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Blog Summary: The Impact of COVID-19 on the Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19709</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>??????? ??????? ???\\r\\r\\nWaiting in a long Que...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID Sentiment                                        CoronaTweet\n",
       "0  22979  Positive  I see all kinds of academics already whipping ...\n",
       "1   9880  Negative  @HenrySmithUK can you raise with Boris please ...\n",
       "2  35761  Negative  It s a confusing odd time for the shopping pub...\n",
       "3  37968  Positive  Blog Summary: The Impact of COVID-19 on the Ca...\n",
       "4  19709   Neutral  ??????? ??????? ???\\r\\r\\nWaiting in a long Que..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7388169f-8fbc-4616-89a7-36df895ac291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows    : 37864\n",
      "number of columns : 3\n",
      "column values     : ['ID', 'Sentiment', 'CoronaTweet']\n",
      "\n",
      "distribution of class lebels : {'Positive': 16602, 'Negative': 14166, 'Neutral': 7096}\n",
      "\n",
      "first row item  : {'ID': 22979, 'Sentiment': 'Positive', 'CoronaTweet': 'I see all kinds of academics already whipping up some #Covid_19 related projects, cfp, syllabi, articles, and blog posts.\\r\\r\\n\\r\\r\\nIÂ\\x92m sittin over here browsing all the food left &amp; tryin to figure out when to go back out to the grocery store. Apparently I donÂ\\x92t do well in pandemic'}\n"
     ]
    }
   ],
   "source": [
    "print (\"number of rows    :\", df_train.shape[0])\n",
    "print (\"number of columns :\", df_train.shape[1])\n",
    "print (\"column values     :\", list(df_train.columns.values))\n",
    "\n",
    "print (\"\\ndistribution of class lebels :\", dict(df_train['Sentiment'].value_counts()))\n",
    "print (\"\\nfirst row item  :\", dict(df_train.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4785e4-73b1-42bc-955a-fda41080f81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f0af457-1cd5-4406-ac53-3f69ba46dc35",
   "metadata": {},
   "source": [
    "### Balance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d22da0-152e-4f7e-b653-fce615a24c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "distribution of class lebels : {'Positive': 16602, 'Neutral': 14192, 'Negative': 14166}\n"
     ]
    }
   ],
   "source": [
    "if balance_dataset == 1:\n",
    "    df_train_net = df_train[df_train.Sentiment == 'Neutral']\n",
    "    df_train = pd.concat([df_train, df_train_net])\n",
    "print (\"\\ndistribution of class lebels :\", dict(df_train['Sentiment'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cebf7b-244f-4661-bae9-ea2f05f7bb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "026559e9-4f69-49ba-badb-542fbe986402",
   "metadata": {},
   "source": [
    "### Converting to Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0ea50a5-146f-4304-be04-572270adab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = list(df_train['Sentiment'])\n",
    "x_train = list(df_train['CoronaTweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb47246-f9ba-4635-97b0-6113e0fa36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (\"number of data points     :\", len(x_train))\n",
    "#print (\"number of class labels    :\", len(y_train))\n",
    "assert(len(x_train) == len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746962e5-5d00-4e78-a7ac-0bc747aee7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a90b3f3-6525-478a-a6d9-102011362c74",
   "metadata": {},
   "source": [
    "### Reading validation data from .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14f91a7d-1c88-408f-9a98-a5fe0ea8766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "__location__ = os.path.realpath(os.path.join(os.getcwd(), \"dataset_corona_sentiment/Corona_validation.csv\"))\n",
    "df_valid = pd.read_csv(__location__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c991d966-80ac-436f-a683-aa228fce9889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows    : 3293\n",
      "number of columns : 3\n",
      "column values     : ['ID', 'Sentiment', 'CoronaTweet']\n",
      "\n",
      "distribution of class lebels : {'Positive': 1444, 'Negative': 1232, 'Neutral': 617}\n",
      "\n",
      "first row item  : {'ID': 7184, 'Sentiment': 'Negative', 'CoronaTweet': 'I reflected on my own consumer behaviour last week and made this list\\r\\r\\nI confess - as much as I feel bad for people who may lose jobs due to the COVID-19, part of me also wish that unethical businesses will no longer be able to operate \"as usual\" unless making changes #time4change https://t.co/63lXRFi82N'}\n"
     ]
    }
   ],
   "source": [
    "print (\"number of rows    :\", df_valid.shape[0])\n",
    "print (\"number of columns :\", df_valid.shape[1])\n",
    "print (\"column values     :\", list(df_valid.columns.values))\n",
    "\n",
    "print (\"\\ndistribution of class lebels :\", dict(df_valid['Sentiment'].value_counts()))\n",
    "print (\"\\nfirst row item  :\", dict(df_valid.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "802ddc22-0616-4132-87c1-5539471cb042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>CoronaTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7184</td>\n",
       "      <td>Negative</td>\n",
       "      <td>I reflected on my own consumer behaviour last ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36363</td>\n",
       "      <td>Negative</td>\n",
       "      <td>I know everyone is getting stir crazy but befo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10423</td>\n",
       "      <td>Negative</td>\n",
       "      <td>I haven t seen gas prices this low since I fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6409</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Only batmeat left on the supermarket shelves\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7015</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Along with health workers, we need to apprecia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID Sentiment                                        CoronaTweet\n",
       "0   7184  Negative  I reflected on my own consumer behaviour last ...\n",
       "1  36363  Negative  I know everyone is getting stir crazy but befo...\n",
       "2  10423  Negative  I haven t seen gas prices this low since I fir...\n",
       "3   6409   Neutral  Only batmeat left on the supermarket shelves\\r...\n",
       "4   7015   Neutral  Along with health workers, we need to apprecia..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927d719-e1e6-4ff0-b9be-a627dcc252c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92824fe6-048f-4613-b4c3-58076c567001",
   "metadata": {},
   "source": [
    "### Perform Lematization & Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87e261-8c03-48a0-91f8-ddbfb8945fe5",
   "metadata": {},
   "source": [
    "##### taken from here: https://medium.com/analytics-vidhya/nlp-tutorial-for-text-classification-in-python-8f19cd17b49e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afc249fb-7236-4200-86a5-31734d27c9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dishantgoyal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/dishantgoyal/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb26c903-3fb2-49db-b31c-ebb5db159dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()  \n",
    "    text=re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    " \n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13f71dcb-20ae-42c2-8051-207917496289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "if use_lemmatizer_stemmer == 1:\n",
    "    df_valid['CleanTweet'] = df_valid['CoronaTweet'].apply(lambda x: finalpreprocess(x))\n",
    "    df_train['CleanTweet'] = df_train['CoronaTweet'].apply(lambda x: finalpreprocess(x))\n",
    "    df_valid.head()\n",
    "    df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a97111-348e-490a-865f-32b42f77567e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09dbd85b-9cad-4a06-814c-ea54f18cdad0",
   "metadata": {},
   "source": [
    "### Converting to Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "223a6987-6933-4db6-992f-c7284d59617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = list(df_valid['Sentiment'])\n",
    "x_valid = list(df_valid['CoronaTweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50c4323d-2d2e-4e69-9684-f15156800843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (\"number of data points     :\", len(x_valid))\n",
    "#print (\"number of class labels    :\", len(y_valid))\n",
    "assert(len(x_valid) == len(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05626ffe-67be-419b-985e-ce72c54f2538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "019e55d1-2cb1-4a43-8ac9-9e05694d19f4",
   "metadata": {},
   "source": [
    "### Convert Data to Frequency Vectors (Bag of Words or Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c3703a-ff50-46c0-9b7f-0e77bd9f9a18",
   "metadata": {},
   "source": [
    "##### fed into scikit learn in this form; numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad572215-8256-4510-b494-a1fd05bed040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords = list(STOPWORDS)\n",
    "\n",
    "if use_bigrams_stopwords == 1:   # consider only words with frequency > 3; else RAM will be done for\n",
    "    vectorizer  = CountVectorizer(analyzer='word', ngram_range=(1, 2), stop_words=stopwords, min_df=3)  \n",
    "elif use_word2vec == 1:\n",
    "    vectorizer  = CountVectorizer()\n",
    "else:\n",
    "    vectorizer  = CountVectorizer()\n",
    "    \n",
    "x_vec       = (vectorizer.fit_transform(x_train + x_valid)).toarray()\n",
    "\n",
    "x_train_vec = x_vec[:len(x_train)]\n",
    "x_valid_vec = x_vec[len(x_train): len(x_train) + len(x_valid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1b86a8c-49c5-4789-9a26-e52cca7103da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bojggl5fi1' 'bojo' 'bojos' 'bokadia_vinita' 'bokakhat' 'bokamotoespn'\n",
      " 'boko' 'bokoharam' 'boksburg' 'bokuto']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "['few', 'and', 'her', \"i'd\", 'between', 'off', 'therefore', 'has', 'k', 'nor']\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.get_feature_names_out()[12345:12355])\n",
    "print (x_train_vec)\n",
    "print (stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739f4519-71a0-4a2f-a370-b827bf28ebac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16b37fcd-ce6d-4a8d-9cc3-f8fa646a5704",
   "metadata": {},
   "source": [
    "### Train Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30b8e4e8-f301-40ee-9db5-ef58b3667e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive' 'Neutral' 'Neutral']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()                             #clf means classification\n",
    "clf.fit(x_train_vec, y_train)\n",
    "\n",
    "print(clf.predict(x_train_vec[2:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6036c5-9e31-4425-8328-bcc5c82004fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfbb8424-ae8c-4816-aabf-ab1bc078cd88",
   "metadata": {},
   "source": [
    "### Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a853162c-3ab7-43b2-a4c2-b2b6a3f303a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid_vec   = x_vec[len(x_train): len(x_train) + len(x_valid)]\n",
    "y_valid_pred  = clf.predict(x_valid_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb360b08-2077-4d91-af80-9ced0a6b4caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR Report         : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.74      0.76      0.75      1444\n",
      "    Negative       0.73      0.74      0.74      1232\n",
      "     Neutral       0.61      0.54      0.57       617\n",
      "\n",
      "    accuracy                           0.71      3293\n",
      "   macro avg       0.69      0.68      0.69      3293\n",
      "weighted avg       0.71      0.71      0.71      3293\n",
      "\n",
      "Confusion Matrix  : \n",
      " [[ 913   90  229]\n",
      " [ 119  332  166]\n",
      " [ 216  126 1102]]\n",
      "\n",
      "Accuracy        :  0.7127239599149712\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "classes = ['Positive', 'Negative', 'Neutral']\n",
    "print(\"PR Report         : \\n\", classification_report(y_valid, y_valid_pred, labels=classes, zero_division=0))\n",
    "print(\"Confusion Matrix  : \\n\", confusion_matrix(y_valid, y_valid_pred))\n",
    "print(\"\\nAccuracy        : \", accuracy_score(y_valid, y_valid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c4930-2391-441e-95f7-00ef37c269f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494470e-b29d-4389-92fe-d2ab9a658452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
